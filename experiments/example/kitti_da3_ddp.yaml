random_seed: 444

dataset:
  type: &dataset_type 'KITTI'
  root_dir: 'data/KITTI'
  # In DDP mode, batch_size is PER-GPU.
  # With 8 GPUs and batch_size=2, effective total batch = 16,
  # which is equivalent to DP with batch_size=16 and 8 GPUs.
  batch_size: 2
  use_3d_center: True
  class_merging: False
  use_dontcare: False
  bbox2d_type: 'anno'   # 'proj' or 'anno'
  meanshape: False      # use predefined anchor or not
  writelist: ['Car']
  random_flip: 0.5
  random_crop: 0.5
  scale: 0.4
  shift: 0.1
  use_da3_depth: True

model:
  type: 'centernet3d'
  backbone: 'dla34'
  neck: 'DLAUp'
  num_class: 3

optimizer:
  type: 'adam'
  # LR stays the same as DP when total batch is preserved.
  # The dp_reference section below lets train_val_ddp.py auto-compute this.
  lr: 0.00125
  weight_decay: 0.00001

lr_scheduler:
  warmup: True  # 5 epoches, cosine warmup, init_lir=0.00001 in default
  decay_rate: 0.1
  decay_list: [90, 120]

trainer:
  max_epoch: 140
  # In DDP mode, gpu_ids is not used (each process uses LOCAL_RANK).
  # Kept here for backward compatibility with DP mode.
  gpu_ids: '0'
  save_frequency: 10 # checkpoint save interval (in epoch)
  # resume_model: 'checkpoints/checkpoint_epoch_70.pth'

# Tells train_val_ddp.py how to auto-compute equivalent DDP settings
# from the original DP config.  If this section is present, batch_size
# and lr above will be OVERRIDDEN to match the DP reference exactly.
distributed:
  enabled: true
  dp_reference:
    total_batch_size: 16
    lr: 0.00125
    num_gpus: 8

distill:
  lambda: 0.5
  loss_type: 'l1'       # 'l1' or 'silog'
  foreground_weight: 5.0


tester:
  type: *dataset_type
  mode: single   # 'single' or 'all'
  checkpoint: 'checkpoints/checkpoint_epoch_140.pth'  # for 'single' mode
  checkpoints_dir: 'checkpoints'  # for 'all' model
  threshold: 0.2  # confidence filter
